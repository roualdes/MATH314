\documentclass{beamer}
\input{BeamOptions.tex}
\begin{document}

<<setup, include=FALSE>>=
options(replace.assign=TRUE, width=40)
opts_knit[["set"]](progress=FALSE)
suppressMessages(library(ggplot2))
@

\title{Analysis of Variance}
\institute{CSU, Chico Math 314} 
\date{\today}
\maketitle

\begin{frame}
  \frametitle{outline}
  \tableofcontents
\end{frame}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{outline}
    \tableofcontents[currentsection]
  \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% frames %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Recap}

\begin{frame}
  \frametitle{recap, Two Sample $t$-test}
  The two sample $t$-test, compares the means of two groups in the hypothesis
  
  \begin{align*}
    H_0: \quad & \mu_1 = \mu_2 \\
    H_A: \quad & \mu_1 \ne \mu_2. \\
  \end{align*}
  
  The test brought us a new statistic with a new standard error.
\end{frame}

\section{ANOVA}
\begin{frame}
  \frametitle{Analysis of Variance, motivation}
  If there were three or more groups, the two sample $t$-test would not work.  We could force the test on the data by comparing two groups at a time, but this has \hyperlink{ref:danger_zone}{dangerous implications}.  We thus require a new statistical method, \textbf{analysis of variance}.
\end{frame}

\subsection{definition}

\begin{frame}
  \frametitle{Analysis of Variance, definition}

  \begin{block}{ANOVA}
    Analysis of variance uses a single hypothesis to check whether the \textit{means} across two or more groups are equal, and uses a new test statistic called $F$ to evaluate this hypothesis.
  \end{block}
  
\end{frame}

\begin{frame}
  \frametitle{ANOVA, hypotheses}
  The ANOVA hypotheses for $k$ groups are
  
  \begin{align*}
    H_0: \quad & \mu_1 = \mu_2 = \mu_3 = \ldots = \mu_k \\
    H_A: \quad & \text{at least one mean is different.}
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{ANOVA, hypothesis examples}
  With ANOVA you can compare the means by groups for many different data sets.
  \begin{itemize}
  \item<2-> mean batting average by position,
  \item<3-> mean movie budget by genre, (or year),
  \item<4-> mean CO2 intake by treatment, or location, (or concentration) 
  \item<5-> mean rem sleep by conservation status,
  \item<6-> mean birth/body weight by family,
  \item<7-> $\ldots$ numerical variable by levels of a categorical variable $\ldots$
  \item<7-> $\ldots$
  \end{itemize}
  
\end{frame}

\subsection{intuition}
\begin{frame}
  \frametitle{ANOVA, intuition}
  Analysis of variance tells us about means by groups, despite its name.  Large variation amongst the groups relative to small variation within the groups indicates different population means.
\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, intution via picture}
  What do you think of these means -- think of variation within and amongst groups?
  <<echo=FALSE, fig.width=6, fig.height=4, fig.align="center">>=
  df <- data.frame(y=c(rnorm(35, 0.1), rnorm(35, 0.05), rnorm(35, 0.15)),
                     f = rep(c("A", "B", "C"), each=35))
  qplot(f, y, data=df, geom="boxplot") + stat_summary(fun.y=mean, colour="red", geom="point", size = 4)
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, intution via picture}
  What do you think of these means -- think of variation within and amongst groups?
  <<echo=FALSE, fig.width=6, fig.height=4, fig.align="center">>=
  df <- data.frame(x=c(rnorm(35, 10), rnorm(35, -2), rnorm(35, 5)),
  f <- rep(c("A", "B", "C"), each=35))
  qplot(f, x, data=df, geom="boxplot") + stat_summary(fun.y=mean, colour="red", geom="point", size = 4)
@
\end{frame}

\subsection{assumptions}
\begin{frame}
  \frametitle{ANOVA, assumptions}
  The ANOVA has three basic assumptions,
  
  \begin{itemize}
  \item<2-> independent observations (amongst and within groups)
  \item<3-> data within each group are nearly normal, and
  \item<4-> the variability of each group is about equal.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, picture}
  The last two assumptions can help us visualize just what is going on.  It starts with box plots by groups -- draw more pictures on board.
  <<echo=FALSE, fig.width=6, fig.height=4, fig.align="center">>=
  df <- data.frame(numerivalvariable = c(rnorm(35, -3), rnorm(35, 3), rnorm(35, 8)), 
                factor = rep(c("A", "B", "C"), each=35))
  qplot(factor, numerivalvariable, data=df, geom="boxplot") + stat_summary(fun.y=mean, colour="red", geom="point", size = 4)
  @
\end{frame}



\subsection{test statistic}
\begin{frame}
  \frametitle{ANOVA, test statistic}
ANOVA calculates one fraction based on two numbers, variation amongst (between) groups and variation within groups.  These two numbers are generally referred to as mean square values.


\only<2->{
\begin{block}{mean squared amongst groups}
  $MSG$ is a strictly positive measure of the variation across all groups, and has $df_G = k-1$ where $k$ represents the number of groups.
\end{block}
}

\only<3->{
\begin{block}{mean squared error}
  $MSE$ is a strictly positive measure of the variation within groups, and has $df_E = n - k$ where $k$ represents the number of groups and $n$ is the sample size.
\end{block}
}
\end{frame}

\begin{frame}
  \frametitle{ANOVA, test statistic}
  The test statistic of ANOVA follows an $F$-distribution,
  
  \[ F = \frac{MSG}{MSE},\]
  and has degrees of freedom associated with the numerator and denominator, $df_G$ and $df_E$, respectively.
\end{frame}

\subsection{F-distribution}

\begin{frame}[fragile]
  \frametitle{$F$-distribution}
  The $F$-distribution is a probability density function over non-negative numbers.  P-values are strictly calculated from the right tail -- hence large $F$ statistics indicate evidence against the null hypothesis.
  
  <<echo=FALSE, fig.width=4, fig.height=2.5, fig.align="center">>=

  p <- ggplot(data.frame(x=c(0,10)), aes(x)) + stat_function(fun=function(x) df(x, 10, 1), n=1001) + labs(x="", y="")
  d <- ggplot_build(p)$data[[1]]
  p + geom_area(data=subset(d, x>7.5), aes(x=x, y=y), fill="black", colour="black") +
        theme(axis.text.y=element_blank(),
                axis.ticks=element_blank())
  @
\end{frame}

\section{Example}

\begin{frame}[fragile]
  \frametitle{ANOVA, example I}
  Are baseball players paid on average differently by position?
  \begin{align*}
    H_0: \quad & \mu_{catcher} = \mu_{dh} = \mu_{first} = \ldots = \mu_{third} \\
    H_A: \quad & \text{at least one mean salary is different.}
  \end{align*}
  with $\alpha = 0.05$.
\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, example I}
Plot the data!
<<>>=
suppressMessages(library(ggplot2))
mlb <- read.csv("https://roualdes.us/data/mlb.csv")
p <- qplot(position, salary, data=mlb, geom="boxplot") + 
    theme(axis.text.x=element_text(angle=45, hjust=1))
@
\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, example}
  <<echo=FALSE, fig.width=6, fig.height=4, fig.align="center">>=
  p
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, in \texttt{R}}
  The \texttt{R} code to run ANOVA
  <<results="hide">>=
  model <- lm(salary{\textasciitilde}position, data=mlb)
  anova(model)
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, in \texttt{R}}
  The tilde $\sim$ is meant to be read as, predict the left hand side with the right hand side.  Hence, we read the following as, predict salary by different levels of position.
  
  <<eval=FALSE>>=
  model <- lm(salary{\textasciitilde}position, data=mlb)
  @
  
  We thus need to ensure that the variable \texttt{position} is a categorical/factor variable.
  <<>>=
  is.factor(mlb[,"position"])
  @

\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, example output}
  The $F$-statistic and p-value are the most important pieces of information to extract from an ANOVA table.  Print output tables from \texttt{R} with 
  <<>>=
  anova(model)
  @
\end{frame}

\begin{frame}
  \frametitle{ANOVA, example conclusion}
  Because the p-value $=\Sexpr{round(anova(model)[["Pr(>F)"]][1], 4)}< \alpha = 0.05$ we reject the null in favor of the alternative.  There is sufficient evidence to claim that the mean salaray of baseball players varies by position.
\end{frame}

\section{Multiple Comparisons -- dangerous implications}
\label{ref:danger_zone}

\subsection{idea}
\begin{frame}
  \frametitle{Multiple Comparisons}
  Running multiple tests on pairs of variables is called \textbf{multiple comparisons} and increases the Type $1$ Error rate.  For instance

\begin{itemize}
\item<2-> Suppose you have data across a categorical variable with $k(=8)$ levels, and you want to compare the means across groups.  What do you do?
\item<3-> I'll tell you what not to do $\ldots$ immediately compare all pairwise combinations of the $k$ groups.  That would result in $\Sexpr{8*7/2}$ tests/confidence intervals.
\end{itemize}
\end{frame}

\subsection{problem}
\begin{frame}
  \frametitle{Multiple Comparisons, the problem}
Suppose you chose a level of significance of $\alpha = 0.05$.  What's the probability of observing at least one significant result, out of the $28$ tests, just by chance?
\end{frame}

\begin{frame}[fragile]
  \frametitle{Multiple Comparisons, the problem}
  This is just a binomial distribution, $X \sim$ Binomial$(28, 0.05)$, and we want to know $P(X \ge 1)$.
<<>>=
sum(dbinom(1:28, 28, 0.05)) # experiment-wise error
@ 
\end{frame}

\begin{frame}
  \frametitle{Multiple Comparisons, solution}
  The \textbf{Bonferroni correction} uses a smaller significance, calculated from the significance level you originally chose.
  
  \begin{block}{Bonferroni correction}
    Given a pre-chosen significance level $\alpha$, the Bonferroni correction specifies a new significance level to use
    \[ \alpha^* = \alpha / K, \]
    where $K = {k \choose 2}$ and $k$ is the number of groups.
  \end{block}
\end{frame}

\subsection{example}



\begin{frame}[fragile]
  \frametitle{\texttt{ape::carnivora}, average brain weight}
Suppose we want to compare morphological characteristics across some carnivorous families.
<<fig.width=6, fig.height=4, fig.align="center">>=
suppressMessages({library(dplyr)
    library(ape)
    data(carnivora)})
carnivs <- filter(carnivora,
                  !(Family %in% c("Ailuridae",
                                  "Procyonidae",
                                  "Viverridae")))
p <- qplot(Family, SB, data=carnivs, geom="boxplot",
           ylab="Brain weight (g)")
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{\texttt{ape::carnivora}, average brain weight}
We of course first plot the data.
<<echo=FALSE, fig.width=6, fig.height=4, fig.align="center">>=
p
@ 
\end{frame}

\begin{frame}
\frametitle{\texttt{ape::carnivora}, average brain weight}
Set up hypothesis test.
\begin{align*}
  H_0: \quad & \mu_c = \mu_f = \mu_h = \mu_m = \mu_u \\
  H_A: \quad & \text{at least one mean is different.}
\end{align*}
with $\alpha = 0.01$.  
\end{frame}

\begin{frame}[fragile]
  \frametitle{\texttt{ape::carnivora}, average brain weight}
  Fit the model.
  <<>>=
  fit <- lm(SB{\textasciitilde}Family, data=carnivs)
  anova(fit)
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{\texttt{ape::carnivora}, average brain weight}
  Because the p-value is tiny, we reject $H_0$ in favor of the alternative.  We basically know which mean is different from the rest, but we still need to use the Bonferroni correction if we want to investigate all pairwise comparisons.
\end{frame}

\begin{frame}[fragile]
  \frametitle{\texttt{ape::carnivora}, average brain weight}
  Bonferroni correction implemented across $k$ groups.
  <<>>=
  # Bonferroni correction
  (k <- length(unique(carnivs[,"Family"])))
  K <- choose(k, 2)
  (astar <- 0.01/K) # new significance level
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{\texttt{ape::carnivora}, average brain weight}
  Now we can make pairwise comparisons.  Have to subset data, again, down to two families of immediate interest.
  <<>>=
  carns1 <- filter(carnivs,
                    Family %in% c("Hyaenidae", "Ursidae"))
  t1 <- t.test(SB{\textasciitilde}Family, data=carns1,
                 conf.level=1-astar)
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{\texttt{ape::carnivora}, average brain weight}
Just Hyaenidae and Ursidae.
  <<>>=
  t1 
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{\texttt{ape::carnivora}, average brain weight}
  Or subset original data again down to two different families.
  <<>>=
  carns2 <- filter(carnivs,
                    Family %in% c("Canidae", "Felidae"))
  t2 <- t.test(SB{\textasciitilde}Family, data=carns2,
                 conf.leve=1-astar)
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{\texttt{ape::carnivora}, average brain weight}
  Just Canidae and Felidae.
  <<>>=
  t2
  @
\end{frame}

\begin{frame}
  \frametitle{Quick Note on ANOVA}
It is possible to reject the null hypothesis using ANOVA and then to not subsequently identify differences in the pairwise comparisons.  This does not invalidate the ANOVA conclusion.  It only means we have not been able to successfully identify which groups differ in their means.
\end{frame}

\subsection{take away}
\begin{frame}
  \frametitle{Multiple Comparisons, take away}
Upfront multiple comparisons inadvertently increases Type I Error rates.  In the real world, you'll likely be in one of two possible positions
  \begin{itemize}
  \item<1-> Formal modelling
    \begin{itemize}
    \item<1-> Need make multiple comparisons with one test, use ANOVA
    \item<2-> only if reject $H_0$: Bonferroni correction\footnote{Bonferroni is an easy correction to use, but is just one of many, e.g.\ \texttt{?TukeyHSD}.} on $\alpha$ and make pairwise comparisons
    \end{itemize}
 
  \item<3-> Prediction
    \begin{itemize}
    \item<3-> use Bonferroni correction up front
    \end{itemize}
  \end{itemize}
\end{frame}

\section{ANOVA Details}

\begin{frame}[fragile]
  \frametitle{recap, ANOVA}
Last time we subset the data set \texttt{ape::carnivora} down to just five families.
<<>>=
suppressMessages({library(ape)
    library(dplyr)
    library(ggplot2)})
data(carnivora)
carnivs <- filter(carnivora,
                 !(Family %in% c("Ailuridae",
                                 "Procyonidae",
                                 "Viverridae")))
@   
\end{frame}

\begin{frame}[fragile]
  \frametitle{recap, ANOVA}
  Then ran ANOVA.
<<results="hide">>=
mod <- lm(SB{\textasciitilde}Family, data=carnivs)
anova(mod)
@ 
\end{frame}

\begin{frame}
  \frametitle{ANOVA, details}
  ANOVA is all about breaking up the response variable, in general denoted by $Y$, into component pieces made up of the explanatory variable(s).  The slide above breaks up the response variable brain weight into an intercept term, and then group means (of sorts) for the remaining $k-1$ levels of Family.
\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, details graphed}
  We think this, but $\ldots$
    <<echo=FALSE, fig.height=4, fig.width=6, fig.align="center">>=
    qplot(Family, SB, data=carnivs, ylab="Brain weight") +
            stat_summary(fun.y=mean, colour="red", geom="point", size=4)
  @
\end{frame}

\begin{frame}
  \frametitle{ANOVA, model}
  The general ANOVA\footnote{This is the most common model used to fit ANOVA, but it is hardly the most intuitive.} model is

  \[ Y_i = \alpha + \beta_1 X_{i,1} + \ldots + \beta_{k-1} X_{i,k-1} + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2), \]

for $i=1, \ldots, n$.  Details

\begin{itemize}
\item<2-> $\alpha$ is called the intercept
\item<3-> $X_{i,j}$ equals $1$ or $0$ dependent on the $i$th observation
\item<4-> $k-1$ levels of the categorical explanatory variable $X_j$ get their own estimated value $\beta_j$
  \begin{itemize}
  \item<5-> $\beta_j$ is difference between the respective group mean and the intercept
  \end{itemize}

\end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, details in \texttt{R}}
Calling the function \texttt{coefficients} on the linear model returns the intercept and $k-1$ values $\beta_j$.  Each group's mean is actually the sum, $\hat{\alpha} + \hat{\beta}_j$ relative to the specific group, $j$, of interest.

<<>>=
coefficients(mod)
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{ANOVA, details in \texttt{R}}
Compare coefficients to the group means

<<>>=
carnivs %>%
    group_by(Family) %>%
    summarise(m=mean(SB))
@
\end{frame}

\begin{frame}
  \frametitle{ANOVA, details in math}
  The model predicts the mean value of $Y_i$, $\mu_{Y|X}$, dependent on the group that observation $Y_i$ came from.  For instance, suppose observation $i$ is a member of Felidae,

  \begin{align*}
    \hat{Y}_i = E(Y_i|X) & = E(Y_i | Felidae) \\
                       & = \hat{\alpha} + \hat{\beta}_1 (X_{i,1}==1) + \ldots + \hat{\beta}_{k-1} (X_{i,k-1} == 0) \\    
                       & = \hat{\alpha} + \hat{\beta}_1. \\
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{ANOVA, details in math}
  The model says that each observation is the sum of the intercept, the appropriate $\beta_j$, and some left over piece, called the \textbf{residuals}, denoted by $\epsilon_i$,

\[ Y_i = \alpha + \beta_1 X_{i,1} + \ldots + \beta_{k-1} X_{i,k-1} + \epsilon_i. \]

The \textbf{residuals} make up the difference between the observation and the sum of the intercept and each $\beta_j$.
\end{frame}

\begin{frame}
  \frametitle{ANOVA, residuals}
  \begin{block}{residuals/errors}
    The theoretical (or random) residuals/errors are often denoted by $\epsilon_i$ in statistical models.  Once observed, after fitting the model, we define the residuals as $e_i = y_i - \hat{y}_i$.
  \end{block}

\begin{exampleblock}{Note}
The difference in the notation between $\epsilon_i$ and $e_i$ is similar to that with random variables: $X$ denotes a random variable, not yet observed, and $x$ is the no longer random value that $X$ took on.  Here, $e_i$ denotes the observed value of the random variable $\epsilon_i$.  
\end{exampleblock}
\end{frame}

\section{Take Away}

\begin{frame}
  \frametitle{Take away}
  \begin{itemize}
  \item ANOVA breaks up the mean of the response variable into components
    \begin{itemize}
    \item<2-> components are specified by a categorical explanatory variable
    \end{itemize}

  \item<3-> Normality assumption is only necessary for small sample sizes
    \begin{itemize}
    \item<4-> interest lies in means $\Rightarrow$ Central Limit Theorem
    \end{itemize}

  \item<5-> equal variance assumption qualitatively checked with plots
    \begin{itemize}
    \item<6-> there exists more formal ways to check this $\ldots$
    \end{itemize}

  \item<7-> you are best bet to ensure independent observations
  \item<8-> multiple comparisons is a silent killer

  \end{itemize}
\end{frame}

\end{document}
