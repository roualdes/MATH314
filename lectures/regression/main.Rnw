\documentclass{beamer}
\input{BeamOptions.tex}

\begin{document}

<<setup, include=FALSE>>=
options(replace.assign=TRUE, width=40)
opts_knit$set(progress=FALSE)

library(ggplot2)
library(dplyr)
suppressMessages(library(MASS))
lmData <- function(n, int=-1, sd=1, slope=pi, from=0, to=10) {
    x = seq(from=from, to=to, length.out=n)
    y = int + slope*x + rnorm(n, sd=sd)
    data.frame(x=x, y=y)
}

nlmData <- function(n, sign=1, sd=10, from=0, to=5) {
    x = seq(from=from, to=to, length.out=n)
    y = sign*exp(x) + rnorm(n, sd=sd)
    data.frame(x=x, y=y)
}
@

\title{Simple Linear Regression}
\institute{CSU Chico, Math 314} 
\date{\today}
\maketitle

\begin{frame}
  \frametitle{outline}
  \tableofcontents
\end{frame}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{outline}
    \tableofcontents[currentsection]
  \end{frame}
}

\section{Recap}

\begin{frame}
  \frametitle{Recap, Take 1: ANOVA}
  ANOVA breaks up means of response variable $Y$ by levels of one categorical variable.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Recap, Take 2: Scatterplots}
 Scatterplots are  a graphical description of two numerical variables; consider Darwin's finch data
<<echo=FALSE, fig.align="center", fig.height=2.5, fig.width=2.5, cache=TRUE>>=
link <- "https://roualdes.us/data/finches.csv"
finch <- read.csv(link)
p <- qplot(middletoelength, winglength, data=finch,
           xlab="Middle toe length (mm)",
           ylab="Wing length (mm)")

p
@   
\end{frame}

\begin{frame}
  \frametitle{Recap, Take 2: Scatterplots}
  We used some keywords to describe scatterplots.
  \begin{itemize}
  \item<2-> associated or not.
  \item<3-> direction: possitive or negative association.
  \item<4-> structure: linear or nonlinear.
  \end{itemize}
\end{frame}

\section{Correlation}


\subsection{definition}

\begin{frame}
  \frametitle{Correlation, definition}
  \textbf{Correlation} is denoted by $R$ and is the numeric analogue of the words above.

  \begin{block}{correlation}
    Correlation, which always takes values between $-1$ and $1$, describes the strength of the linear relationship between two variables.
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Correlation, notes}
Notes on correlation
  \begin{itemize}
  \item<1-> More accurate name is Pearson correlation coefficient.
  \item<2-> Describes linear relationships only.
  \item<3-> Bounded by $-1$ and $1$.
  \item<4-> The value $0$ denotes no association.
  \item<5-> The sign dictates directionality.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Correlation, math}
  Mathematically, (Pearson) correlation is defined as

\[ R = \frac{1}{n-1} \sum_{i=1}^n \left( \frac{x_i-\bar{x}}{s_x} \right) \cdot \left(\frac{y_i-\bar{y}}{s_y} \right). \]

In \texttt{R} we should just use the function \texttt{cor}.
<<>>=
with(finch,
     cor(middletoelength, winglength))
@ 
\end{frame}

\begin{frame}
  \frametitle{Correlation, nonparametric}
  The nonparametric version of Pearson correlation is \textbf{Spearman's rank correlation coefficient} or \textbf{Spearman's rho}, denoted by $\rho$.

  \begin{itemize}
  \item<2-> The value $0$ denotes no association.
  \item<3-> The sign dictates directionality.
  \item<4-> Bounded by $-1$ and $1$.
  \item<5-> Describes \textbf{monotonic} -- this includes linear -- relationships.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Correlation, nonparametric in \texttt{R}}
  In \texttt{R} we can use the function \texttt{cor} and specify the method option,
<<>>=
with(finch,
     cor(middletoelength, winglength, method="spearman"))
@ 
\end{frame}

\begin{frame}
  \frametitle{Correlation, use}
  Pearson correlation is the most common.  That is to say our default assumption is linear relationships.  If there is convincing evidence otherwise, then use Spearman's rho.
\end{frame}

<<echo=FALSE>>=
suppressMessages(library(MASS))
corVars <- function(n, r) mvrnorm(n, c(0,0), matrix(c(1,r,r,1), nrow=2), empirical=TRUE)
@ 

\subsection{examples}
\begin{frame}[fragile]
  \frametitle{Correlation,  example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
    x <- corVars(100, 0.33)
    qplot(x[,1], x[,2], ylab="", xlab="R = 0.33") + theme(text = element_text(size=15))
    @ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Correlation,  example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
  y <- corVars(100, 0.70)
  qplot(y[,1], y[,2], ylab="", xlab="R = 0.70") + theme(text = element_text(size=15))
  @ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Correlation,  example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
  y = corVars(100, 0.95)
  qplot(y[,1], y[,2], ylab="", xlab="R = 0.95") + theme(text = element_text(size=15))
  @ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Correlation,  example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
  y <- corVars(100, 0.99)
  qplot(y[,1], y[,2], ylab="", xlab="R = 0.99") + theme(text = element_text(size=15))
  @ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Correlation,  example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
  y <- corVars(100, 1)
  qplot(y[,1], y[,2], ylab="", xlab="R = 1") + theme(text = element_text(size=15))
  @ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Correlation,  example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
  y <- corVars(100, -1)
  qplot(y[,1], y[,2], ylab="", xlab="R = -1") + theme(text = element_text(size=15))
  @ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Correlation,  example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
  y <- corVars(100, -0.70)
  qplot(y[,1], y[,2], ylab="", xlab="R = -0.70") + theme(text = element_text(size=15))
  @ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Correlation,  example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
  y <- corVars(100, -0.95)
  qplot(y[,1], y[,2], ylab="", xlab="R = -0.95") + theme(text = element_text(size=15))
  @ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Correlation,  example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
    x <- corVars(100, 0)
    qplot(x[,1], x[,2], ylab="", xlab="R = 0") + theme(text = element_text(size=15))
    @ 
\end{frame}

\begin{frame}
  \frametitle{Correlation, (watch out)  example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
  x <- seq(1, 9, length.out=101)
  y <- (x-4)^2+rnorm(101)
  r <- cor(x, y)
  qplot(x, y, ylab="", xlab=bquote(R == .(r))) + theme(text = element_text(size=15))
  @ 
\end{frame}

\begin{frame}
  \frametitle{Correlation, (watch out) example}
  <<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
  x <- seq(0, 10, length.out=101)
  y <- sin(x)+rnorm(101, sd=0.2)
  r <- cor(x, y)
  qplot(x, y, ylab="", xlab=bquote(R == .(r))) + theme(text = element_text(size=15))
  @ 
\end{frame}

\begin{frame}
  \frametitle{Correlation, plots}
  This is another reason plots are so important.
\end{frame}

\section{Simple Linear Regression}
\begin{frame}
  \frametitle{Simple Linear Regression, introduction}
  For all of the appropriately linear correlation examples above, it was easy to think about a line through the data and then ask, ``How closely do the data fall onto that line?''  That line through the data however, has a name and a mathematical definition.
\end{frame}

\subsection{lite example}
\begin{frame}[fragile]
  \frametitle{Simple Linear Regression, plotted}
  The \textbf{least squares line} for Darwin's finch data is plot below in blue.
<<echo=FALSE, fig.height=3, fig.width=3, fig.align="center">>=
p + stat_smooth(method="lm", se=FALSE)
@ 
\end{frame}

\begin{frame}
  \frametitle{Simple Linear Regression, idea}
  Simple linear regression decomposes the response variable $Y$ into three components: 
  \begin{itemize}
  \item<2-> the \textbf{intercept}
    \begin{itemize}
    \item<2-> the value $Y$ takes on when $X$ is equal to $0$; 
    \item<3-> above, the length of a wing when the middle toe length is $0$
    \end{itemize}
  \item<4-> the \textbf{slope}
    \begin{itemize}
    \item<4-> on the explanatory variable $X$
    \item<5-> represents the increase in $Y$ for a unit increase in $X$;
    \item<6-> above, some increase in wing length for every mm increase in the middle toe length
    \end{itemize}
  \item<7-> \textbf{errors/residuals}
    \begin{itemize}
    \item<7-> some left over bits
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Linear Regression, model}
  Given a response variable $Y$ and an explanatory variable $X$, the simple linear regression model is

\[ Y_i = \underbrace{\beta_0}_{\text{intercept}} + \underbrace{\beta_1}_{\text{slope}} X_i + \underbrace{\epsilon_i}_{\text{errors}}, \quad \epsilon_i \sim N(0, \sigma^2), \]

where $i = 1, \ldots, n$.
\end{frame}

\subsection{assumptions}
\begin{frame}
  \frametitle{Simple Linear Regression, assumptions}
  Simple linear regression assmptions
  \begin{itemize}
  \item<2-> Linearity -- the data should show a linear trend.
  \item<3-> Independent observations -- no two points are dependent on each other.
  \item<4-> Constant Variability -- variation of points around least squares line remains roughly constant.
  \item<5-> Normality -- the residuals should be nearly normal.
  \end{itemize}
\end{frame}

\subsection{parameter}

\begin{frame}
  \frametitle{Simple Linear Regression, parameters}
  The population parameters $\beta_0$ and $\beta_1$ are estimated with $\hat{\beta}_0$ and $\hat{\beta}_1$.  These estimates within the linear regression equation are written

\[ E(Y|X) = \hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X. \]

The expected value, or fitted value, of $Y$ is a function of the estimates of the intercept and slope, dependent on some value of $X$.
\end{frame}

\subsection{residuals}
\begin{frame}
  \frametitle{Linear Regression, residuals}
  Not every observation will fall of the least squares line.  The difference between the true observation $Y_i$ and the predicted value of $\hat{Y}_i$ at the $X_i$, is the $i$th residual

\[ e_i = Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_0 + \hat{\beta}_1 X_i) \]
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple Linear Regression, residuals by picture}
Some residuals will be positive and some negative.

<<echo=FALSE, fig.width=3, fig.height=3, fig.align="center", cache=TRUE>>=
w1 <- with(finch, which(middletoelength < 17))[1]
w2 <- with(finch, which(winglength > 77))[1]
fit <- lm(winglength{\textasciitilde}middletoelength, data=finch)
p + stat_smooth(method="lm", se=FALSE) +
      geom_segment(aes(x = finch[w1,"middletoelength"],
                       y = finch[w1,"winglength"],
                       xend = finch[w1,"middletoelength"],
                       yend = sum(coef(fit)*c(1,finch[w1,"middletoelength"])),
                       colour="red")) +
      annotate("text", x = finch[w1,"middletoelength"] + 0.5,
               y = 62.5, label = "e[66]", parse=TRUE) + 
      geom_segment(aes(x = finch[w2,"middletoelength"],
                       y = finch[w2,"winglength"],
                       xend = finch[w2,"middletoelength"],
                       yend = sum(coef(fit)*c(1,finch[w2,"middletoelength"])),
                       colour="red")) +
      annotate("text", x = finch[w2,"middletoelength"]-0.5,
               y = 78.5, label = "e[15]", parse=TRUE) + 
      guides(colour=FALSE)
@ 
\end{frame}

\subsection{estimation}
\begin{frame}
  \frametitle{Simple Linear Regression, best}
  The word ``best'' is cleverly defined and not without debate.  The most common definition of best means the line that minimizes the sum of the squared residuals.  This idea is intuitive.  We are to find the values of $\beta_0$ and $\beta_1$ that

  \begin{itemize}
  \item<2-> take $e_i = Y_i - \hat{Y}_i$, for all $i$,
  \item<3-> square each residual, $e_i^2$, and 
  \item<4-> minimize $\sum_{i=1}^n e_i^2$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Simple Linear Regression}
  Our model implies $Y \sim N(\beta_0 + X\beta_1, \sigma^2)$.  Parameter estimates are found via likelihood.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Simple Linear Regression in \texttt{R}}
In \texttt{R} we use the function \texttt{lm} to fit linear regression.  The format is much the same as that for \texttt{aov}.
<<cache=TRUE>>=
fit <- lm(winglength{\textasciitilde}middletoelength, data=finch)
## summary(fit) # RStudio
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Simple Linear Regression, interpretation}
  The coefficients from the model can be extracted with the function \texttt{coefficients}.

<<cache=TRUE>>=
(beta <- coefficients(fit))
@ 

Thus, our fitted linear model is written as
\[ E(Y|X) = \Sexpr{round(beta[1], 2)} + \Sexpr{round(beta[2], 2)}X. \]
\end{frame}

\section{Example}

\begin{frame}[fragile]
  \frametitle{Elmhurst College Data}
  We'll consider a dataset named  \texttt{elmhurst}.  With these data, we might have the question, ``How is family income related to the amount of gift aid a student receives from the college?''

<<cache=TRUE>>=
url <- "https://roualdes.us/data/elmhurst.csv"
elmhurst <- read.csv(url)
head(elmhurst)
@ 
\end{frame}

\begin{frame}
  \frametitle{Elmhurst College Data}
Step 1? 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Elmhurst College Data}
  Plot the data!
<<fig.align="center", fig.height=3, fig.width=3, cache=TRUE>>=
p <- qplot(family_income, gift_aid, data=elmhurst,
          ylab="Gift aid ($1000)",
          xlab="Family income ($1000)")
with(elmhurst,
     cor(family_income, gift_aid))
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Elmhurst College Data}
  <<fig.align="center", fig.width=3, fig.height=3, cache=TRUE>>=
  p + stat_smooth(method="lm", se=FALSE) # no standard errors plot
  @ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{Elmhurst College Data}
  <<fig.align="center", fig.width=3, fig.height=3, cache=TRUE>>=
  elmReg <- lm(gift_aid {\textasciitilde} family_income, data=elmhurst)
  # summary(elmReg) ## RStudio
  @ 
\end{frame}

<<echo=FALSE, cache=TRUE>>=
beta <- coef(elmReg)
@ 

\begin{frame}[fragile]
  \frametitle{Elmhurst College Data}
  Our estimated linear model looks like

\[ \widehat{aid} = \Sexpr{round(beta[1], 2)} + \Sexpr{round(beta[2], 2)} \times family\_income. \]

How do we interpret this?
\end{frame}

\begin{frame}
  \frametitle{Elmhurst College Data}
  Can we make causal connections from this model?
\end{frame}

\begin{frame}
  \frametitle{Take away}
  \begin{itemize}
  \item<1-> Correlation is a helpful summary statistic between two numerical variables
    \begin{itemize}
    \item<2-> there is a nonparametric version, but its no panacea
    \end{itemize}
  \item<3-> Linear regression fits ``best'' line through scatterplot
    \begin{itemize}
    \item<4-> best means minimized squared residuals
    \item<5-> expected value of response given some value of explanatory
    \item<6-> the assumptions are important, we will return to them many times
    \end{itemize}
  \end{itemize}
\end{frame}

\section{References}
\nocite{Paradis:2004,Wickham:2009,Diez:2015}
\begin{frame}[allowframebreaks]
  \frametitle{references}
  \bibliographystyle{plainnat} \bibliography{../ref}
\end{frame}
\end{document}
